# **Week 1: Python Foundations & Environment Setup**

---

## ğŸ“… **Day 1**

### ğŸ”¹ Topics Covered:

- Introduction â€” About the instructor
- Overview of the course
- Installation and setup of:
  - **Visual Studio Code**
  - **Python**
  - **Git**

---

### ğŸ§  Key Terms

- **Generative AI (Artificial Intelligence)**  
  AI that can **create new content** â€” such as text, images, code, audio, or video â€” rather than just analyze data.

- **LLM (Large Language Model)**  
  A type of AI trained on **massive amounts of text** to predict and generate **human-like language**.

- **Platform**  
  A set of **tools, services, infrastructure, and environments** used to develop, deploy, and manage AI applications.

---

## ğŸ“… **Day 2**

### ğŸ”¹ Topics Covered:

- Recap of Day 1
- Introduction to:
  - **Git** (technology for version control)
  - **GitHub** (platform for hosting code)

---

### ğŸ› ï¸ Git Commands

- `git clone <URL>`  
  Clones a remote repository to your local machine. Use only once for initial setup.

- `git checkout <CommitID>`  
  Switches to a previous version of the code using a commit ID.

- `git checkout main`  
  Returns to the latest version on the main branch.

- `git pull`  
  Pulls the latest code from the remote repository if it has been set up.

---

### ğŸ’» Practice Topics

- Creating commits and pushing code to GitHub
- Python Basics:
  - **Variables**
  - **User input with `input()`**
  - **Conditional statements (`if`, `elif`, `else`)**

### [Homework](https://github.com/justyzas/25-05-28-GEN-AI/blob/main/homework/2025-05-29.MD)

## ğŸ“… **Day 3**

- Questions and Recap of Day 2
- Python operations with strings, numbers, introduction to lists

- Discussing about GEN AI models
- Gen AI models limitations

#### ğŸ”® 1. **Hallucination (Making Things Up)**

- Gen AI models may **generate content that sounds plausible but is completely false**.
- This includes:
  - Incorrect facts
  - Fake references
  - Made-up URLs or studies
- âš ï¸ Critical in fields like medicine, law, or academia.

#### ğŸ“… 2. **Lack of Real-Time or Recent Knowledge**

- Most models are trained on **static data** with a **knowledge cutoff** (e.g., GPT-4 is trained up to late 2023).
- They **do not access the internet** by default.
- This makes them **unreliable for breaking news, live events, or up-to-date info**.

---

#### ğŸ§  3. **No True Understanding or Reasoning**

- LLMs **do not think or understand** like humans.
- They generate text based on **patterns and probabilities**, not logic or comprehension.
- They may fail at:
  - Multi-step logical problems
  - Understanding ambiguous input
  - Reasoning about cause and effect

---

#### ğŸ§¾ 4. **Memory and Context Limitations**

- Models have a **maximum context length** (e.g., 4k, 8k, 128k tokens).
- If you input too much text, earlier content may be **truncated or forgotten**.
- They do **not persist memory** between sessions unless designed with tools like memory APIs or databases.

#### ğŸŒ 5. **Bias and Toxicity**

- Models are trained on **internet-scale data**, which includes **biases, stereotypes, and harmful language**.
- They may:
  - Reflect societal biases
  - Reinforce stereotypes
  - Generate offensive or inappropriate output if not carefully prompted or filtered

### 6. It may fail on very simple and specific tasks, for example, counting 'r's in word "strawberry"

### 7. It may fail in image generation while trying to add some text. Characters may seem distorted.

### 8. Hosting LLM may seem very expensive in resources

## ğŸ¤– **AI Model Types**

---

### ğŸ§  1. **Vision LLMs**

#### ğŸ” What They Do:

- Process and understand **images or visual input**.
- Combine **language + vision** to answer questions about pictures, describe images, or interact **multimodally**.

#### ğŸ“Œ Typical Tasks:

- **Image captioning**: â€œDescribe this image.â€
- **Visual question answering**: â€œWhatâ€™s the man holding in the picture?â€
- **Multimodal interaction**: â€œRead the text in this screenshot and explain it.â€

#### ğŸ§ª Examples in Ollama:

- `llava`, `bakllava`, `gpt-4o` (if supported), `blip`, `clip`

#### âš™ï¸ How They Differ:

- Expect an **image as input**, alongside or instead of text.
- Cannot be used meaningfully for **language-only tasks** like embeddings.

---

### ğŸ› ï¸ 2. **Tools LLMs**

#### ğŸ” What They Do:

LLMs configured to work with **external tools**, like:

- Calculators
- Web search
- Code execution
- APIs

#### ğŸ“Œ Typical Tasks:

- â€œWhatâ€™s 1249123 divided by 7.3?â€ â†’ Calls a **calculator tool**
- â€œGet the current weather in Berlinâ€ â†’ Calls a **weather API**
- **Agent workflows** (multi-step reasoning)

#### ğŸ§ª Examples in Ollama:

- Tool-using variants of LLMs integrated with:
  - **LangChain**
  - **AutoGPT-like flows**
  - **Function-calling models**

#### âš™ï¸ How They Differ:

- They donâ€™t â€œknowâ€ the answers â€” they **perform actions or call helpers**.
- May rely on **additional code or environment**.

  - Ollama **doesnâ€™t handle tool execution** natively â€” it **serves the LLM**; the toolchain is external.

  ***

### ğŸ§  3. **Language LLMs (Text-only Models)**

#### ğŸ” What They Do:

These are the most common LLMs â€” trained to **understand and generate human-like language**.

#### ğŸ“Œ Typical Tasks:

- Chatbots and virtual assistants
- Code generation and debugging
- Summarization, translation, rewriting
- Creative writing or brainstorming

#### ğŸ§ª Examples in Ollama:

- `llama3`, `mistral`, `gemma`, `phi`, `tinyllama`

#### âš™ï¸ How They Differ:

- Work only with **text input and output**.
- Great for general-purpose use, but **not multimodal** (canâ€™t process images).
- Often the most optimized and resource-efficient models in Ollama.

---

### ğŸ§© 4. **Embedding Models**

#### ğŸ” What They Do:

These models **convert text into vectors (embeddings)** â€” numerical representations that capture semantic meaning.

#### ğŸ“Œ Typical Tasks:

- Semantic search (e.g., â€œfind documents similar to this queryâ€)
- Clustering similar texts
- Vector databases (e.g., Pinecone, FAISS)
- Text classification and similarity scoring

#### ğŸ§ª Examples in Ollama:

- Embedding variants of `mistral`, `bge`, or smaller transformer-based models

#### âš™ï¸ How They Differ:

- They donâ€™t generate language like a chatbot.
- Their job is **representational** â€” not conversational.
- Output is a **vector of numbers**, not words or sentences.

---

- **Ollama model installation** and **running** on **local machine**

## ğŸ“… **Day 4: Lists, Loops & Prompt Engineering**

---

### ğŸ” **Topics Covered**

- âœ… Recap of Day 3
- ğŸ“ Review of Homework
- ğŸ“‹ Working with **Lists**:
  - Creating and accessing elements
  - Slicing syntax (`list[start:stop:step]`)
  - Common methods: `.append()`, `.remove()`, `.sort()`, etc.
- ğŸ **Python Debugging**:
  - Using `print()` statements
  - Using breakpoints in VS Code
- ğŸ” **Loops**:
  - `for` and `while` loops
  - Looping through lists and ranges
  - `break`

---

### ğŸ§  **Key Terms & Concepts**

- **Token**  
  The smallest unit into which text data can be broken down for an AI model to process.

- ğŸ’° **Token Cost Tip**  
  _Input is often cheaper than output_ â€” models charge less to receive a prompt than to generate a response.

- **Prompt Engineering**  
  A skill and practice for designing, refining, and optimizing **prompts** to effectively guide large language models (LLMs) in performing tasks or answering questions.  
  Useful for:

  - Improving accuracy
  - Reducing hallucinations
  - Enhancing creative output

- **API Key**  
  A **secret identifier** used to authenticate with an external API service (e.g., OpenAI, Hugging Face).  
  âš ï¸ **CAUTION:** Never share your API key publicly â€” it can be misused and expose you to unwanted costs.

---

## ğŸ“… **Day 5: Dates, Functions & Prompt Engineering**

- âœ… Recap of Day 4
- Homework review
- Python libraries importing
- Datetime library
- Defining and calling functions
- Importing from another file/module
- ğŸ¤– What is **Prompt Engineering** and why it matters
- Removing local model from cache of ollama: `ollama rm model:<b>b`
- âœï¸ Prompt crafting techniques:
  - Setting clear instructions
  - Using role assignments
  - Providing examples or constraints
- ğŸ§ª Live comparisons of bad vs good prompts
- ğŸ§‘â€ğŸ’» Optional: Sending prompts to local LLM

- **Zero-shot Prompting**  
  Asking the model to do a task **without showing examples** â€” relies on general instructions, that model learned in the training.

- **Few-shot Prompting**

## ğŸ“… **Day 7: Dates, Functions & Prompt Engineering & first AI call via API**
